#Refactored 22 Dec 2023

#This script assembles two tables, "geochrons.all" and "geochrons". The former contains all geochronology entries (dates) from collection units meeting the following criteria: (1) in North America (2) 14C age \ < 30, 000 14Cybp (\~34, 445 calybp) (3) has a vertebrate fossil dataset

#The latter ("geochrons") obeys (1) and (2) but is filtered by taxon: (3a) contains at least one specimen assigned to a taxon in Rodentia, Lagomorpha, or Lipotyphla.

#Both these tables contain the following columns: "geochronid", "sampleid", "geochrontypeid", "agetypeid", "age", "errorolder", "erroryounger", "infinite", "delta13c", "labnumber", "materialdated", "notes", "analysisunitid", "datasetid", "depenvtid", "collectionunitid", "siteid", "sitename"

#This version pulls its data from the Neotoma 2.0 API using functions in the "neotoma2" library.

#download list of sites by type and convert to dataframe

required.packages <- c("neotoma2","here","httr","jsonlite","dplyr","rlist")
uninstalled.packages <- required.packages[!(required.packages %in% installed.packages()[,"Package"])]
if(length(uninstalled.packages) > 0) install.packages(uninstalled.packages)
sapply(required.packages, function(x) eval(parse(text = paste("require(",x,")"))))

i_am("workflow/scripts/getdata.R")
#load file and skip download if possible
if (exists("NAm_sites_chron")) rm(NAm_sites_chron)
if (exists("NAm_sites_vert")) rm(NAm_sites_vert)
if (file.exists(here("workflow/data/NAm_sites.RData"))) {
  if (file.mtime(here("workflow/data/NAm_sites.RData")) < 
      seq(Sys.Date(), length = 2, by = paste0(c("-","1 month"),collapse = ""))[2]) {
        load(here("workflow/data/NAm_sites.RData"))}
  } else {
    if (file.exists(here("workflow/data/NAm_sites_dls.RData"))) {
      if(file.mtime(here("workflow/data/NAm_sites_dls.RData")) < 
         seq(Sys.Date(), length = 2, by = paste0(c("-","1 month"),collapse = ""))[2]) {
            load(here("workflow/data/NAm_sites_dls.Rdata"))}}
  }
#otherwise download and filter sites
#define North America
NAm_wkt <- readLines(here("workflow/data/NAm_wkt.txt"))
#download all sites in North America polygon with "vertebrate fauna" or "geochronologic" datasets
if (!exists("NAm_sites_vert")) {
  print("Downloading North American vertebrate sites...")
  NAm_sites_vert <- get_sites(loc = NAm_wkt, datasettype = "vertebrate fauna", all_data = T, limit = 9999999)
}
if (!exists("NAm_sites_chron")) {
  print("Downloading North American sites with geochronologic dates...")
  NAm_sites_chron <- get_sites(loc = NAm_wkt, datasettype = "geochronologic", all_data = T, limit = 9999999)
}
chronsites <- getids(NAm_sites_chron);chronsites$type <- "chron"
vertsites <- getids(NAm_sites_vert);vertsites$type = "vert"
save(NAm_sites_chron, NAm_sites_vert, vertsites, chronsites, file = "NAm_sites.RData")
rm(NAm_sites_vert, NAm_sites_chron)


#download sites with both dataset types
data <- get_sites(as.numeric(intersect(chronsites$siteid, vertsites$siteid)), all_data = TRUE)
siteinfo <- show(data)
#get dataset info, split by dataset types
datasets <- datasets(data)
datasets.info <- show(datasets)
datasets.info <- split(datasets.info, datasets.info$datasettype)

#filter down to *collections* with vert & chron data (not just sites)
vert_chron_sites <- getids(data) #returns site, collection, and dataset IDs for all sites in argument
#all vertebrate dataset IDs and associated siteID & collunitID
vert_datasets <- subset(vert_chron_sites, vert_chron_sites$datasetid %in% datasets.info$`vertebrate fauna`$datasetid)
#all chron dataset IDs and associated siteID & collunitID
chron_datasets <- subset(vert_chron_sites, vert_chron_sites$datasetid %in% datasets.info$geochronologic$datasetid)
#all collection unit IDs associated with both dataset types
vert_chron_collunits <- intersect(unique(vert_datasets$collunitid), unique(chron_datasets$collunitid))
#add the site and dataset IDs back
vert_and_chron <- subset(vert_chron_sites, vert_chron_sites$collunitid %in% vert_chron_collunits)
#rename column and convert to integer
vert_and_chron <- data.frame(apply(vert_and_chron, 2, as.numeric))
colnames(vert_and_chron)[which(colnames(vert_and_chron) == "collunitid")] <- "collectionunitid"
#add the dataset type
vert_and_chron$datasettype <- NA
vert_and_chron[which(vert_and_chron$datasetid %in% datasets.info$`vertebrate fauna`$datasetid),"datasettype"] <- "vertebrate"
vert_and_chron[which(vert_and_chron$datasetid %in% datasets.info$geochronologic$datasetid),"datasettype"] <- "geochron"
#save to disk
write.csv(vert_and_chron,file = here("workflow/data/vert_and_chron.csv"),row.names = F)

##check how many collections were dropped
print(paste0(c("Collections dropped due to not having both vertebrate and geochron datasets:", 
        apply(vert_chron_sites[which(!vert_chron_sites$collunitid %in% vert_and_chron$collectionunitid), ], 
              2, function(x) length(unique(x)))[2]), collapse = " "))

#download further data

##use Tilia calls to assemble list of samples
checkRecent <- function(x) {
  if (max(as.Date(x$recdatemodified), na.rm = T) > seq(Sys.Date(), length = 2, by = "-1 month")[2]) {
    return("TRUE")
  } else return("FALSE")
}
get_from_tilia <- function(value, param, meth) {
  paramstring <- paste0(apply(data.frame(param,as.character(value)),1,function(x) paste0(x,collapse="=")),collapse="&_")
  url <- paste0(c("https://tilia.neotomadb.org/api/?method=ti.", meth, "&_", paramstring), collapse = "")
  obj <- try(GET(url), silent = TRUE)
  return(fromJSON(content(obj,as = "text")))
}

if (exists("samples.ti")) {if (checkRecent(samples.ti) == F) {rm(samples.ti)}}
if (!exists("samples.ti")) {
  samples.ti <- vector(mode = "list",length = nrow(vert_and_chron))
  print(paste0(c("Downloading geochron sample data for",nrow(vert_and_chron),"datasets..."),collapse = " "))
  I <- 1; J <- 100
  while (I <= nrow(vert_and_chron)){
    for (i in I:J) {
      temp <- get_from_tilia(value = vert_and_chron$datasetid[i], 
                             param = "datasetid", 
                             meth = "getgeochronanalysisunitsbydatasetid")$data
      if (length(temp) == 0 & !is.null(dim(temp))) { 
        temp <- data.frame(sampleid = NA,collectionunitid = NA,
                           analysisunitid = NA,analysisunitname = NA,
                           depth = NA,thickness = NA) 
        samples.ti[[i]] <- data.frame(datasetid = vert_and_chron$datasetid[i], temp)
        }
    }
    save(samples.ti,file = here("workflow/data/samples_ti_temp.RData"))
    if (i %% 1000 == 0) {print(paste0(c(i,"downloaded"),collapse=" "))}
    I <- i + 1; J <- i + 100
    if (J > nrow(vert_and_chron)) {J <- nrow(vert_and_chron)} 
  }
  print(paste0(c("Done.",length(which(sapply(samples.ti,function(x) is.na(x$sampleid[1])))),"of",length(samples.ti),"failed."),collapse = " "))
  #getting sampleid and analysisunitid by datasetid
} #run time: ~15 minutes
which(sapply(samples.ti,function(x) is.na(x$sampleid)))
samples.ids <- distinct(list.stack(samples.ti)[, c("datasetid", "sampleid", "analysisunitid")])
samples.ids <- subset(samples.ids,!is.na(samples.ids$sampleid))

#join all together into table, downloading other tables
#columns: (vert_and_chron) siteid, collectionunitid, datasetid
#         (samples) sampleid, analysisunitid
#         (collectionunits) depenvtid
#         (sites) sitename 
#         (geochronology) all columns
#temp1 <- merge(vert_and_chron, samples.ids, by = "datasetid")

##old version downloads tables directly
#temp1 <- merge(vert_and_chron, samples.ids, by = "datasetid")
#if (exists("geochronology.table")) {if (checkRecent(geochronology.table) == F) {rm(geochronology.table)}}
#if (!exists("geochronology.table")) {print("Downloading table 'geochronology'...")
#  geochronology.table <- distinct(get_table("geochronology", limit = 99999999))}
#temp2 <- merge(temp1, geochronology.table[, 1:12], by = "sampleid")
#if (exists("collectionunits.table")) {if (checkRecent(collectionunits.table) == F) {rm(collectionunits.table)}}
#if (!exists("collectionunits.table")) {print("Downloading table 'collectionunits'...")
#  collectionunits.table <- distinct(get_table("collectionunits", limit = 99999999))}
#temp3 <- merge(temp2, collectionunits.table[, c("collectionunitid", "depenvtid")], by = "collectionunitid")
#geochrons.all <- distinct(merge(temp3, siteinfo[, c("siteid", "sitename")], by = "siteid"))
##rename agetypeid levels with text
#geochrons.all$agetypeid <- as.factor(geochrons.all$agetypeid)
#agetypeids <- get_table("agetypes", limit = 9999999)
#levels(geochrons.all$agetypeid) <- agetypeids[match(levels(geochrons.all$agetypeid), agetypeids$agetypeid), "shortagetype"]
##rename geochrontypeid levels with text
#geochrons.all$geochrontypeid <- as.factor(geochrons.all$geochrontypeid)
#geochrontypeids <- get_table("geochrontypes", limit = 9999999)
#levels(geochrons.all$geochrontypeid) <- geochrontypeids[match(levels(geochrons.all$geochrontypeid), geochrontypeids$geochrontypeid), "geochrontype"]

#new version gets mostly from Tilia
if (exists("geochrons.ti")) {if (checkRecent(geochrons.ti) == F) {rm(geochrons.ti)}}
if (!exists("geochrons.ti")) {
  chron <- subset(vert_and_chron,vert_and_chron$datasettype == "geochron")
  geochrons.ti <- vector(mode = "list",length = nrow(chron))
  print(paste0(c("Downloading geochron data for",nrow(chron),"datasets..."),collapse = " "))
  I <- 1; J <- 100
  while (I <= nrow(chron)){
    for (i in I:J) {
      temp <- get_from_tilia(value = chron$datasetid[i], 
                             param = "datasetid", 
                             meth = "getgeochronbydatasetid")$data
      if (length(temp) == 0) { temp <- data.frame(geochronid = NA, geochrontypeid = NA, geochrontype = NA, 
                                                  agetype = NA, depth = NA, thickness = NA, analysisunitid = NA, 
                                                  analysisunitname = NA, age = NA, errorolder = NA, erroryounger = NA,
                                                  infinite = NA, labnumber = NA, materialdated = NA, notes = NA) }
      geochrons.ti[[i]] <- data.frame(datasetid = chron$datasetid[i], temp)
    }
    save(geochrons.ti,file = here("workflow/data/geochrons_ti_temp.RData"))
    if (i %% 500 == 0) {print(paste0(c(i,"downloaded"),collapse=" "))}
    I <- i + 1; J <- i + 100
    if (J > nrow(chron)) {J <- nrow(chron)} 
  }
  print(paste0(c("Done.",length(which(sapply(geochrons.ti,function(x) is.na(x$geochronid[1])))),"of",length(geochrons.ti),"failed."),collapse = " "))
}
geochrons.ti <- distinct(list.stack(geochrons.ti)[, c(1,2,4,5,8:16)])
geochrons.ti <- subset(geochrons.ti,!is.na(geochrons.ti$geochronid))

if (exists("collections.ti")) {if (checkRecent(collections.ti) == F) {rm(collections.ti)}}
if (!exists("collections.ti")) {
  colls <- unique(vert_and_chron$collectionunitid)
  collections.ti <- vector(mode = "list",length = length(colls))
  print(paste0(c("Downloading data for",length(colls),"collections..."),collapse = " "))
  I <- 1; J <- 100
  while (I <= length(colls)) {
    for (i in I:J) {
      temp <- get_from_tilia(value = colls[i], 
                             param = "collectionunitid", 
                             meth = "getcollunitbyid")$data
      if (length(temp) == 0) { temp <- data.frame(collectionunitid = NA,handle = NA,siteid = NA,colltypeid = NA,
                                                  depenvtid = NA,collunitname = NA,colldate = NA,colldevice = NA,
                                                  gpslatitude = NA,gpslongitude = NA,gpsaltitude = NA,gpserror = NA,
                                                  waterdepth = NA,substrateid = NA,slopeaspect = NA,slopeangle = NA,
                                                  location = NA,notes = NA) }
      collections.ti[[i]] <- data.frame(datasetid = colls[i], temp)
    }
    save(collections.ti,file = here("workflow/data/collections_ti_temp.RData"))
    if (i %% 500 == 0) {print(paste0(c(i,"downloaded"),collapse = " "))}
    I <- i + 1; J <- i + 100
    if (J > length(colls)) {J <- length(colls)} 
  }
  print(paste0(c("Done.",length(which(sapply(collections.ti,function(x) is.na(x$collectionunitid[1])))),"of",length(collections.ti),"failed."),collapse = " "))
}
collections.ti <- distinct(list.stack(collections.ti)[,c("collectionunitid","depenvtid")])

if (exists("geochronology.table")) {if (checkRecent(geochronology.table) == F) {rm(geochronology.table)}}
if (!exists("geochronology.table")) {print("Downloading table 'geochronology'...")
  geochronology.table <- distinct(get_table("geochronology", limit = 99999999))}

#merge all together
a <- distinct(inner_join(subset(vert_and_chron,vert_and_chron$datasettype == "geochron"),
                         distinct(samples.ids[,c(1,3)]),by = "datasetid"))
b <- distinct(right_join(a, geochrons.ti, by = "analysisunitid"))
c <- distinct(inner_join(b,data.frame(siteid = as.numeric(siteinfo$siteid),siteinfo = siteinfo$sitename),by = "siteid"))
d <- distinct(inner_join(c, collections.ti, by = "collectionunitid"))
e <- distinct(left_join(d,distinct(geochronology.table[,c("geochronid","sampleid","delta13c")]),by="geochronid"))

geochrons.all <- e[,c("siteid", "collectionunitid", "sampleid", "datasetid.x", "analysisunitid", 
                      "geochronid", "geochrontype", "agetype", "age", "errorolder", "erroryounger", 
                      "infinite", "delta13c", "labnumber", "materialdated", "notes", "depenvtid", "siteinfo")]
colnames(geochrons.all) <- c("siteid", "collectionunitid", "sampleid", "datasetid", "analysisunitid", 
                             "geochronid", "geochrontypeid", "agetypeid", "age", "errorolder", "erroryounger", 
                             "infinite", "delta13c", "labnumber", "materialdated", "notes", "depenvtid", "sitename")

print(paste(nrow(geochrons.all), "dates found before filtering by taxonomy and age", collapse = " "))
write.csv(geochrons.all, here("workflow/data/dates_for_analysis_allvert.csv"), row.names = F)

#filter by taxa and age 
##filter geochrons.all to only <30ka
colls.30ka <- subset(geochrons.all, geochrons.all$age <= 30000 | 
                       (geochrons.all$age - geochrons.all$erroryounger) <= 30000)$collectionunitid
#how many rows do we want to drop?
nrow(geochrons.all) - nrow(subset(geochrons.all, geochrons.all$collectionunitid %in% colls.30ka))
#use dates from collection units containing <30ka dates
geochrons <- subset(geochrons.all, geochrons.all$collectionunitid %in% colls.30ka) 
print(paste(nrow(geochrons), "dates from collection units with ages =< 30ka", collapse = " "))

##get list of small mammal taxa
##taxon list defined as "smalltaxa" in filter_taxa.Rmd
smalltaxa <- try(read.csv("workflow/data/smallmammaltaxa.csv"), silent = T)
if (!exists("smalltaxa")) {
  source("filter_taxa.R")
}

#select datasetids of only vertebrate datasets from collections in geochrons.all
rm(i)
if (exists("taxonids")) {if (any(!geochrons$datasetid %in% taxonids$datasetid)) rm(taxonids)}
if (!exists("taxonids")) {
  vert.datasets <- unique(intersect(subset(vert_and_chron,
                                  vert_and_chron$collectionunitid %in% geochrons.all$collectionunitid)[, "datasetid"],
                           datasets.info$`vertebrate fauna`$datasetid))
  temp <- vector(mode = "list",length = length(vert.datasets))
  print("Downloading taxon IDs...")
  if (exists("i")) {I <- i + 1; J <- round(i + 51,-2)} else {I <- 1; J <- 100}
  pb = txtProgressBar(min = I - 1, max = length(vert.datasets), initial = I)
  while (I <= length(vert.datasets)){
    for (i in I:J) {
      verts.temp <- try(get_from_tilia(vert.datasets[i],param = "datasetid",meth = "getdatasetvariables")$data) 
      vals <- smalltaxa[na.omit(match(verts.temp$taxoncode,smalltaxa$taxoncode)), "taxonid"]
      temp[[i]] <- data.frame(datasetid = rep(vert.datasets[i],length(vals)),
                              taxonid = vals)
      setTxtProgressBar(pb, i)
    }
    I <- i + 1; J <- i + 100
    if (J > length(vert.datasets)) {J <- length(vert.datasets)} 
  }
  close(pb)
  taxonids <- distinct(rlist::list.stack(temp))
  write.csv(taxonids,"workflow/data/taxonids.csv",row.names = F)
}

smallmamm.colls <- unique(subset(vert_and_chron, vert_and_chron$datasetid %in% taxonids$datasetid)$collectionunitid)
smallmamm.sites <- unique(subset(vert_and_chron, vert_and_chron$datasetid %in% taxonids$datasetid)$siteid)

##filter geochrons.all to only dates from collections containing small mammals
##should this be filtered to site level instead?
geochrons <- subset(geochrons, geochrons$collectionunitid %in% smallmamm.colls)
print(paste(nrow(geochrons), "dates found from small mammal collections =< 30ka"))

geochrons$rejected <- FALSE
geochrons[grepl("[Dd]ate rejected",geochrons$notes) | grepl("[Dd]ate too",geochrons$notes) | grepl("[Cc]ontamination probable",geochrons$notes),"rejected"] <- TRUE
geochrons <- distinct(geochrons)

#print counts
print(c("Unique values:",
        apply(geochrons[, c("geochronid", "analysisunitid", "collectionunitid")], 2, function(x) length(unique(x)))))
print(c(round(table(geochrons$agetypeid)[1]/nrow(geochrons)*100), "% of dates already calibrated"))
##save
write.csv(geochrons, here("workflow/data/dates_for_analysis_smallmamm.csv"), row.names = F)


#remove objects other than geochrons, geochrons_all, smalltaxa
##large tables created in script
rm(siteinfo, datasets, datasets.info, vert_chron_sites, vert_datasets, chron_datasets, vert_chron_collunits, vert_and_chron, chronsites, vertsites, vert.datasets, samples.ti)
##functions and some other rubbish
rm(pb, I, NAm_wkt, checkRecent, getvariables.bydataset)
